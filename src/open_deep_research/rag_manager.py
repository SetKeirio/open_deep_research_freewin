"""
RAG Manager - —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º–∞–º–∏.
"""

import asyncio
import hashlib
import pickle
from pathlib import Path
from typing import List, Optional, Dict
from datetime import datetime

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig

from open_deep_research.configuration import Configuration


class RAGManager:
    """Singleton –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è RAG —Å–∏—Å—Ç–µ–º."""
    
    _instance = None
    _rag_systems: Dict[str, 'RAGSystem'] = {}
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def get_rag_system(self, config_key: str) -> 'RAGSystem':
        """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å RAG —Å–∏—Å—Ç–µ–º—É –ø–æ –∫–ª—é—á—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        if config_key not in self._rag_systems:
            self._rag_systems[config_key] = RAGSystem()
        return self._rag_systems[config_key]
    
    async def search(
        self,
        query: str,
        file_paths: List[str],
        k: int = 5,
        config: Optional[RunnableConfig] = None
    ) -> str:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ RAG —Å–∏—Å—Ç–µ–º—É."""
        configurable = Configuration.from_runnable_config(config)
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_hash = hashlib.md5(
            f"{sorted(file_paths)}".encode()
        ).hexdigest()
        
        rag_system = self.get_rag_system(config_hash)
        return await rag_system.search(query, file_paths, k, config)


class RAGSystem:
    """–û—Å–Ω–æ–≤–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞."""
    
    def __init__(self, cache_dir: str = "./.rag_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.vectorstore = None
        self.file_hashes = {}
        self.last_update = None
    
    def _get_file_hash(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ö—ç—à —Ñ–∞–π–ª–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è."""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return "missing"
    
    def _get_cache_key(self, file_paths: List[str]) -> str:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á –∫—ç—à–∞."""
        hashes = [self._get_file_hash(fp) for fp in file_paths]
        return hashlib.md5("|".join(sorted(hashes)).encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫—ç—à–∞."""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                    self.vectorstore = data['vectorstore']
                    self.file_hashes = data['file_hashes']
                    self.last_update = data['timestamp']
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω RAG –∏–Ω–¥–µ–∫—Å –∏–∑ –∫—ç—à–∞: {cache_key}")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
        return False
    
    def _save_to_cache(self, cache_key: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à."""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        try:
            data = {
                'vectorstore': self.vectorstore,
                'file_hashes': self.file_hashes,
                'timestamp': datetime.now()
            }
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω RAG –∏–Ω–¥–µ–∫—Å –≤ –∫—ç—à: {cache_key}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def _needs_rebuild(self, file_paths: List[str]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å."""
        if not self.vectorstore:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ —Ñ–∞–π–ª—ã
        current_hashes = {fp: self._get_file_hash(fp) for fp in file_paths}
        return current_hashes != self.file_hashes
    
    def _build_rag_index(self, file_paths: List[str]):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏–∑ —Ñ–∞–π–ª–æ–≤."""
        all_documents = []
        
        print(f"üî® –°—Ç—Ä–æ–∏–º RAG –∏–Ω–¥–µ–∫—Å –¥–ª—è {len(file_paths)} —Ñ–∞–π–ª–æ–≤...")
        
        for file_path in file_paths:
            path = Path(file_path)
            
            if not path.exists():
                print(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                continue
            
            try:
                # –í—ã—á–∏—Å–ª–∏—Ç—å —Ö—ç—à —Ñ–∞–π–ª–∞
                file_hash = self._get_file_hash(file_path)
                self.file_hashes[file_path] = file_hash
                
                # –ó–ê–ì–†–£–ó–ö–ê –§–ê–ô–õ–ê - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
                print(f"üìñ –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª: {path.name}")
                
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –≤—Ä—É—á–Ω—É—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                print(f"üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"üìù –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤: {content[:500]}...")
                
                # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤—Ä—É—á–Ω—É—é
                docs = [Document(
                    page_content=content,
                    metadata={
                        "source": str(file_path),
                        "filename": path.name,
                        "file_size": len(content)
                    }
                )]
                
                all_documents.extend(docs)
                print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ {path.name}")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
                import traceback
                traceback.print_exc()
        
        if not all_documents:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!")
        
        # –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100,
            separators=["\n\n\n", "\n\n", "\n", ". ", " ", ""],
            length_function=len,
            is_separator_regex=False
        )
        chunks = splitter.split_documents(all_documents)
        print(f"üìä –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        for i, chunk in enumerate(chunks[:3]):  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 3 —á–∞–Ω–∫–∞
            print(f"  –ß–∞–Ω–∫ {i+1}: {len(chunk.page_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"  –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {chunk.page_content[:200]}...")
        
        # –°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        print("üß† –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            chunk_size=100
        )
        
        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vectorstore = FAISS.from_documents(chunks, embeddings)
        print(f"‚úÖ –ü–æ—Å—Ç—Ä–æ–µ–Ω RAG –∏–Ω–¥–µ–∫—Å —Å {len(chunks)} –≤–µ–∫—Ç–æ—Ä–∞–º–∏")
        
        # –û–±–Ω–æ–≤–∏—Ç—å –≤—Ä–µ–º—è
        self.last_update = datetime.now()
    
    async def search(
        self, 
        query: str, 
        file_paths: List[str], 
        k: int = 5,
        config: Optional[RunnableConfig] = None
    ) -> str:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫."""
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à –∏ –ø–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        cache_key = self._get_cache_key(file_paths)
        
        if not self._load_from_cache(cache_key) or self._needs_rebuild(file_paths):
            self._build_rag_index(file_paths)
            self._save_to_cache(cache_key)
        
        # –í—ã–ø–æ–ª–Ω–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        print(f"üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: '{query}' (k={k})")
        
        try:
            # –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            docs = self.vectorstore.similarity_search(query, k=k)
            
            if not docs:
                return "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª–∞—Ö."
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = []
            results.append(f"## üß† –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞")
            results.append(f"**–ó–∞–ø—Ä–æ—Å:** '{query}'")
            results.append(f"**–§–∞–π–ª–æ–≤ –ø–æ–∏—Å–∫–∞–Ω–æ:** {len(file_paths)}")
            results.append(f"**–°–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç—Ä—ã–≤–∫–∏:**\n")
            
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get('filename', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                score = doc.metadata.get('score', '–ù/–î')
                
                result = f"""
### üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç {i}: {source}
**–û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏:** {score if score != '–ù/–î' else '–í—ã—Å–æ–∫–∞—è'}
**–§–∞–π–ª –∏—Å—Ç–æ—á–Ω–∏–∫–∞:** `{doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}`

**–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:**
{doc.page_content[:800]}{'...' if len(doc.page_content) > 800 else ''}

---
"""
                results.append(result)
            
            # –î–æ–±–∞–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–∏—Å–∫–∞
            results.append(f"\n**–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            return "\n".join(results)
            
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}"


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
_rag_manager_instance = None

def get_rag_manager() -> RAGManager:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞ RAG."""
    global _rag_manager_instance
    if _rag_manager_instance is None:
        _rag_manager_instance = RAGManager()
    return _rag_manager_instance